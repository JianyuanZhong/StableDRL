desc: GSM8K-SPG-SNIS-gsm8k-math-snis-gsm8k-only

network_kwargs:
    # Updated class_name to use the LLaDOUModelLM wrapper defined in lladou_spg_snis.py
    class_name: networks.llada_svpo.LLaDASVPO.from_pretrained
    pretrained_model_name_or_path: GSAI-ML/LLaDA-8B-Instruct # REPLACE WITH YOUR MODEL PATH
    trust_remote_code: true
    torch_dtype: bfloat16

tokenizer_kwargs:
    class_name: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: GSAI-ML/LLaDA-8B-Instruct

# Configuration for the generation phase (Rollouts)
infer_kwargs:
    # The training loop handles generation internally using these parameters.
    # Removed: func_name
    num_generations: 8
    repeat_times: 2

    # Parameters for LLaDA-style iterative decoding (used by generate_spg)
    max_steps: 128       # Corresponds to diffusion_steps
    gen_length: 256      # Corresponds to max_completion_length
    block_size: 32       # Updated from block_length; used for generation and masking config
    temperature: 0.9
    # Removed: gamma

data_loader_kwargs:
  class_name: dataloaders.math.load_mixed_datasets_and_reward
  math_path: qwedsacf/competition_math
  gsm8k_path: openai/gsm8k
  batch_size: 1
  num_workers: 4
  mixing_strategy: interleave
  sampling_weights: [0.0, 1.0]
  max_rows_per_dataset: 100000000

# Configuration for the loss calculation (Used to initialize SPGConfig)
loss_kwargs:
    # The training loop handles loss calculation internally using these parameters.
    # Removed: func_name

    # SPG Bound parameters
    spg_beta: 1.5        # Corresponds to eubo_beta
    spg_omega: 0.5       # Corresponds to mix_weight (implies logp_estimation='mix')

    # Monte Carlo Estimation parameters
    num_mc_samples: 2    # Corresponds to num_t
    p_mask_perturb: 0.15 # Corresponds to p_mask_prompt
    # Removed: block_size, noise_schedule

    # New AIS/SNIS parameters
    use_snis: true          # Enable Self-Normalized Importance Sampling stabilization
    ais_clip_iw: 5.0        # Clipping threshold for importance weights (linear scale)

optimizer_kwargs:
    class_name: torch.optim.AdamW
    lr: 2.0e-6
    betas: [0.9, 0.99]
    eps: 1.0e-8
    weight_decay: 0.1

lr_scheduler_kwargs:
    class_name: training.utils.lr_scheduler.linear_decay
    total_steps: 2000

training_args:
    func_name: training.training_loop_spg_snis.training_loop
    run_dir: ./runs # REPLACE WITH YOUR RUN DIR
    total_steps: 2000
    loss_scaling: 1.
    grad_accumulation: 4
    max_grad_norm: 0.2
    seed: 113
    step_per_tick: 1
    snapshot_ticks: 25
    # Including standard args that were truncated in the prompt:
    state_dump_ticks: 500
    val_ticks: 1
    precision: bf16 # Aligned with network_kwargs torch_dtype
    activation_checkpointing: 'whole_layer'

    # GRPO specific arguments required by the updated training loop
    num_iterations: 2     # Number of inner optimization steps per generation cycle
    random_masking: true

